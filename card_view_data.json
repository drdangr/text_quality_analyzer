[
    {
        "paragraph_id": 0,
        "text": "онтологическая геометрия",
        "signal_strength": 0.788,
        "complexity": 1.0
    },
    {
        "paragraph_id": 1,
        "text": "Как я вас, заголовком? Потужно, не? \nНу а шо, мне одному страдать?",
        "signal_strength": 0.799,
        "complexity": 0.38
    },
    {
        "paragraph_id": 2,
        "text": "В общем, дальше для специалистов будет смешное, для нас, дятлов и чайников — ликбез.",
        "signal_strength": 0.802,
        "complexity": 0.478
    },
    {
        "paragraph_id": 3,
        "text": "(Это в основном про меня, если что. \nЯ это всё пишу себе , чтобы уложить в голове. Ну и вы полистаете, если интересно. \nЕсли в комменты занесёт кого-то взрослого — не бросайте в меня тапочком, пожалуйста. \nЯ, как в том анекдоте: «не настоящий сварщик, я так, на стройке маску нашёл».)",
        "signal_strength": 0.813,
        "complexity": 0.544
    },
    {
        "paragraph_id": 4,
        "text": "Так. О страшных словах, которыми оперируют Лелик и Болик, а также время от времени разные инфо-цыгане, которые пишут о нейросетях.",
        "signal_strength": 0.805,
        "complexity": 0.547
    },
    {
        "paragraph_id": 5,
        "text": "Токены.\nТокены-токены-токены — все пишут о каких-то токенах, в них измеряют длину контекстного окна, а чем оно длиннее, говорят, тем лучше. (Кто б сомневался, ага.) Но об этом позже.",
        "signal_strength": 0.814,
        "complexity": 0.506
    },
    {
        "paragraph_id": 6,
        "text": "Токен — часть текста. Обчыно - меньше слова. Помнишь, как в детстве: ма-ма мы-ла ра-му. Что-то вроде этого — каждое слово разбивается на токены.",
        "signal_strength": 0.846,
        "complexity": 0.307
    },
    {
        "paragraph_id": 7,
        "text": "Насколько я вкурил, токенами могут быть как слова целиком, так и их части, и даже знаки препинания. В общем, токен — это минимальная удобная для модели единица текста.",
        "signal_strength": 0.837,
        "complexity": 0.524
    },
    {
        "paragraph_id": 8,
        "text": "Нафига? Для сжатия.\nГаврюша (эти все “нейросеть”, “большая языковая модель”, писать каждый раз целиком неудобно, а “ии” — как-то тупо) настоящими словами, чтоб вы понимали, вообще не пользуется.",
        "signal_strength": 0.807,
        "complexity": 0.641
    },
    {
        "paragraph_id": 9,
        "text": "Она в основном оперирует токенами и их эмбедингами.",
        "signal_strength": 0.834,
        "complexity": 0.908
    },
    {
        "paragraph_id": 10,
        "text": "Вставай, вставай, @имярек, ну да.\nЭмбединг.",
        "signal_strength": 0.802,
        "complexity": 0.662
    },
    {
        "paragraph_id": 11,
        "text": "Не ссы. Это мы ещё в этот лес вообще не вошли, так на пороге тусим.",
        "signal_strength": 0.782,
        "complexity": 0.088
    },
    {
        "paragraph_id": 12,
        "text": "Так, я сразу оговорюсь: я довольно тупой человек, и мне больно учиться. \nПоэтому, как работает современная гаврюша, я не знаю и даже не собираюсь туда лезть. Мне надо понять, как на этой машинке доехать из пункта А в пункт Б. Так что придётся выяснить, что такое руль и педали. А что там под капотом — пока не будем трогать.",
        "signal_strength": 0.787,
        "complexity": 0.61
    },
    {
        "paragraph_id": 13,
        "text": "Едем дальше. \nЭмбединг, значит.",
        "signal_strength": 0.803,
        "complexity": 0.338
    },
    {
        "paragraph_id": 14,
        "text": "Представь себе указатель: «Лондон — 100 км».\nЭто вектор. Одномерный. С длиной.\nВ смысле того, что он работает в одном направлении, то есть в одном измерении — туда, 100 км вдоль его стрелки.\nТо есть мы бы так и записали Лондон (100).",
        "signal_strength": 0.796,
        "complexity": 0.362
    },
    {
        "paragraph_id": 15,
        "text": "Дальше. Повесим его на столб, пусть ещё указывает не только как далеко, но и где этот вектор в координатах север-юг, восток-запад. Например, 100 км на север и 50 на запад.\nХоба. Он уже двумерный.\nИ мы бы записали: Лондон (100, 50). \nНаша вся география, как вы помните из школы, помещается во всего-навсего два измерения.",
        "signal_strength": 0.774,
        "complexity": 0.408
    },
    {
        "paragraph_id": 16,
        "text": "Теперь усложним задачу: пусть нам не просто надо добраться до точки на карте, а еще и определиться с высотой. Как с доставкой на этаж. Представь похожий указатель, который висит на столбе и показывает на Гришино окно на пятом этаже. Это уже три измерения: туда-сюда и вверх. \nИ мы бы записали Григорий: 35 метров туда, 35 сюда, 5 этадей вверх. \n(35,35,5).Три измерения.",
        "signal_strength": 0.784,
        "complexity": 0.521
    },
    {
        "paragraph_id": 17,
        "text": "Дальше ты должен налить себе на два пальца и представить, что этих измерений больше трёх.\nНа трезвую голову это упражнение могут исполнить только наркоманы и учащиеся факультета прикладной математики.\nИ то… Впрочем, о пересекающихся множествах мы поговорим сильно позже. Если доживём.",
        "signal_strength": 0.78,
        "complexity": 0.594
    },
    {
        "paragraph_id": 18,
        "text": "Но я помогу: представь, что тебе надо доставить посылку Гришеному младшему, причем завтра, в 15.30, после школы. \nБерем, стало быть, географические координаты предположим (35,35), добавим этаж (5), добавим кому из семьи выдать - младшему, пусть он будет первым по счету в семье, пишем (1), и когда (15:30). \nИтого: посылка младшему должна прибыть в (35,35,5,1,15:30)",
        "signal_strength": 0.783,
        "complexity": 0.663
    },
    {
        "paragraph_id": 19,
        "text": "Не хочу тебя пугать, дорогой, но ты сейчас увидел пример пятимерной координаты.\nКак ты, мм? Держишься?",
        "signal_strength": 0.798,
        "complexity": 0.41
    },
    {
        "paragraph_id": 20,
        "text": "Ну, вот видишь. Пятое измерение открыли, и ничего, нормально. \n(А Лелик, между прочим, оперирует в 12 228 мерном. И не жалуется.)",
        "signal_strength": 0.786,
        "complexity": 0.458
    },
    {
        "paragraph_id": 21,
        "text": "Так. Теперь дальше:\nУ каждого токена есть этот самый вектор ебического количества измерений. Это называется векторизация токенов.",
        "signal_strength": 0.828,
        "complexity": 0.585
    },
    {
        "paragraph_id": 22,
        "text": "Теперь соберись, будет больно:\nВо-первых, у каждого токена не просто есть свой вектор в этом многомерном пространстве — то бишь эмбединг. Он ещё и предварительный, гаврюша по ходу прожевывания смыслов его уточняет.\nВо-вторых, у него ещё может быть дополнительный вектор, указывающий, где этот токен находится в общей последовательности токенов в тексте. \nНу а как иначе. Надо ж их как-то друг за дружкой правильно выстроить, да?",
        "signal_strength": 0.853,
        "complexity": 0.587
    },
    {
        "paragraph_id": 23,
        "text": "Если ты ещё не ушёл (а я б так и сделал на твоём месте, честно говоря) — это ещё не всё.",
        "signal_strength": 0.785,
        "complexity": 0.2
    },
    {
        "paragraph_id": 24,
        "text": "Этот самый эмбединг, то есть указатель, в последствии, когда гаврюша соображает, куда ты её послал, может быть не только у отдельного токена, а у любой смысловой единицы: у слова, абзаца и целого предложения.",
        "signal_strength": 0.83,
        "complexity": 0.754
    },
    {
        "paragraph_id": 25,
        "text": "Понятно, что чем более крупная смысловая единица прицеплена к одному- единственному эмбедингу, тем этот указатель менее ценный.\n— Дорогой, как тебе сегодняшний спектакль?\n— Хуета.\nЭмбединг как бы есть, но практического толку от него мало.",
        "signal_strength": 0.815,
        "complexity": 0.644
    },
    {
        "paragraph_id": 26,
        "text": "Ну и кое-что надо уточнить: у каждой гаврюши своя размерность эмбедингов. \nЕдиная на всю модель. \nИ в этом многомерном бульоне у нее внутри плавают все смыслы.",
        "signal_strength": 0.835,
        "complexity": 0.522
    },
    {
        "paragraph_id": 27,
        "text": "Чем качественнее текст превращён в эти многомерные ёжики из указателей, тем точнее с ними можно дальше работать.",
        "signal_strength": 0.776,
        "complexity": 0.604
    },
    {
        "paragraph_id": 28,
        "text": "Грубо (ох, простите меня, Лелик и Болик, наверно навру сейчас с три короба, но ладно), процесс ээээ.. укладки смысла фразы «закрыл замок на замок, чтобы замок не замок» в гаврюшу выглядит так:",
        "signal_strength": 0.796,
        "complexity": 0.621
    },
    {
        "paragraph_id": 29,
        "text": "Разбиение на токены, векторизация этих токенов, добавление векторов, в какой последовательности эти токены образуют слова, превращение слов в эмбединги.",
        "signal_strength": 0.85,
        "complexity": 0.874
    },
    {
        "paragraph_id": 30,
        "text": "Причём современные гаврюши не просто каждому «замок» дают один и тот же эмбединг, а развешивают свой для каждого следующего «замок», исходя из контекста и семантики, прости господи.",
        "signal_strength": 0.822,
        "complexity": 0.821
    },
    {
        "paragraph_id": 31,
        "text": "Лелик, например, внутри сам развешивает эти смыслы, согласно его представлении о прекрасном.. а вот если ты решил, как я, дурак, поиграться в построение своего кастомного Дживса, (по науке это называется RAG - Retrieval-Augmented Generation, то есть генерация смыслов, дополненных внешним источником.. твоим, то есть, источником.. эээ ладно, об этом отдельный текст надо писать..) короче если ты хочешь, чтобы модель понимала смысл не от фонаря, а на базе того, что ты ей подсунешь, то для поиска смыслов твой текст надо перед превращением в набор эмбедингов еще немного обработать.",
        "signal_strength": 0.817,
        "complexity": 1.0
    },
    {
        "paragraph_id": 32,
        "text": "И тут появляется ещё одно страшное слово: чанкинг. Чанк-инг.\n(Для запоминания: как челюсти клацают: чанк-чанк-чанк, смысл режется на куски)",
        "signal_strength": 0.808,
        "complexity": 0.494
    },
    {
        "paragraph_id": 33,
        "text": "То есть если ты хочешь, чтобы один «замок» отправился в сторону архитектуры, следующий — в штуки, которые не дают дверям распахнуться, третий — опять в архитектуру (причём он ещё и тот же самый «замок», что был вначале), а последний — в свойства влажности, нужно эту фразу порезать на чанки.\nЧанк — слово, чанк — следующее слово. \nТогда каждому «замок» можно развесить по своему эмбедингу.\nТам всякие про это есть хитрости, как гаврюше, в отличие от китайского студента, понять, какому «замок» — куда. Но не суть.",
        "signal_strength": 0.837,
        "complexity": 0.637
    },
    {
        "paragraph_id": 34,
        "text": "Причём, это еще не всё: можно еще и всей фразе давать целый эмбединг и тогда всю фразу в целом можно обозначить общим смыслом (как в истории про спектакль) и отправить в «поговорки», а если можно навешивать больше одного эмбединга на единицу смысла (а кто запретит) — ещё и в анекдоты про китайцев, учащих русский язык, и в «часть текста Дана, который вынес мне сегодня мозг».",
        "signal_strength": 0.818,
        "complexity": 1.0
    },
    {
        "paragraph_id": 35,
        "text": "Фух.\nВсё, выдыхай, бобёр.\nДо следующей серии.",
        "signal_strength": 0.784,
        "complexity": 0.365
    },
    {
        "paragraph_id": 36,
        "text": "Я наврал, это в ней я уже навалю про семантическую географию (бляха, я теперь могу этим термином прямо между глаз засветить кому-то) и ещё разложу, почему она бывает линейная и нелинейная.\n(Ага, в следующий раз будешь сразу скроллить на котиков.)",
        "signal_strength": 0.798,
        "complexity": 0.58
    }
]