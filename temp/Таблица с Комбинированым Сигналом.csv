Text,signal_strength,signal_norm,semantic_function,signal_combined
онтологическая геометрия,0.788,0.16666666666666785,шум,0.0
"Как я вас, заголовком? Потужно, не? 
Ну а шо, мне одному страдать?",0.799,0.29761904761904745,юмор / ирония / сарказм,0.29761904761904745
"В общем, дальше для специалистов будет смешное, для нас, дятлов и чайников — ликбез.",0.802,0.3333333333333339,юмор / ирония / сарказм,0.3333333333333339
"(Это в основном про меня, если что. 
Я это всё пишу себе , чтобы уложить в голове. Ну и вы полистаете, если интересно. 
Если в комменты занесёт кого-то взрослого — не бросайте в меня тапочком, пожалуйста. 
Я, как в том анекдоте: «не настоящий сварщик, я так, на стройке маску нашёл».)",0.813,0.4642857142857135,лирическое отступление,0.4642857142857135
"Так. О страшных словах, которыми оперируют Лелик и Болик, а также время от времени разные инфо-цыгане, которые пишут о нейросетях.",0.805,0.3690476190476204,раскрытие темы,0.5535714285714306
"Токены.
Токены-токены-токены — все пишут о каких-то токенах, в них измеряют длину контекстного окна, а чем оно длиннее, говорят, тем лучше. (Кто б сомневался, ага.) Но об этом позже.",0.814,0.4761904761904763,раскрытие темы,0.7142857142857144
"Токен — часть текста. Обчыно - меньше слова. Помнишь, как в детстве: ма-ма мы-ла ра-му. Что-то вроде этого — каждое слово разбивается на токены.",0.846,0.8571428571428577,пояснение на примере,1.071428571428572
"Насколько я вкурил, токенами могут быть как слова целиком, так и их части, и даже знаки препинания. В общем, токен — это минимальная удобная для модели единица текста.",0.837,0.75,раскрытие темы,1.125
"Нафига? Для сжатия.
Гаврюша (эти все “нейросеть”, “большая языковая модель”, писать каждый раз целиком неудобно, а “ии” — как-то тупо) настоящими словами, чтоб вы понимали, вообще не пользуется.",0.807,0.3928571428571441,раскрытие темы,0.5892857142857162
"Она в основном оперирует токенами и их эмбедингами.
Вставай, вставай, @имярек, ну да.
Эмбединг.
Не ссы. Это мы ещё в этот лес вообще не вошли, так на пороге тусим.
Так, я сразу оговорюсь: я довольно тупой человек, и мне больно учиться. 
Поэтому, как работает современная гаврюша, я не знаю и даже не собираюсь туда лезть. Мне надо понять, как на этой машинке доехать из пункта А в пункт Б. Так что придётся выяснить, что такое руль и педали. А что там под капотом — пока не будем трогать.",0.858,1.0,раскрытие темы,1.5
"Едем дальше. 
Эмбединг, значит.",0.803,0.3452380952380949,связующий переход,0.3452380952380949
"Представь себе указатель: «Лондон — 100 км».
Это вектор. Одномерный. С длиной.
В смысле того, что он работает в одном направлении, то есть в одном измерении — туда, 100 км вдоль его стрелки.
То есть мы бы так и записали Лондон (100).",0.796,0.26190476190476275,метафора / аналогия,0.26190476190476275
"Дальше. Повесим его на столб, пусть ещё указывает не только как далеко, но и где этот вектор в координатах север-юг, восток-запад. Например, 100 км на север и 50 на запад.
Хоба. Он уже двумерный.
И мы бы записали: Лондон (100, 50). 
Наша вся география, как вы помните из школы, помещается во всего-навсего два измерения.",0.774,0.0,метафора / аналогия,0.0
"Теперь усложним задачу: пусть нам не просто надо добраться до точки на карте, а еще и определиться с высотой. Как с доставкой на этаж. Представь похожий указатель, который висит на столбе и показывает на Гришино окно на пятом этаже. Это уже три измерения: туда-сюда и вверх. 
И мы бы записали Григорий: 35 метров туда, 35 сюда, 5 этадей вверх. 
(35,35,5).Три измерения.",0.784,0.11904761904761862,метафора / аналогия,0.11904761904761862
"Дальше ты должен налить себе на два пальца и представить, что этих измерений больше трёх.
На трезвую голову это упражнение могут исполнить только наркоманы и учащиеся факультета прикладной математики.
И то… Впрочем, о пересекающихся множествах мы поговорим сильно позже. Если доживём.",0.78,0.07142857142857117,метафора / аналогия,0.07142857142857117
"Но я помогу: представь, что тебе надо доставить посылку Гришеному младшему, причем завтра, в 15.30, после школы. 
Берем, стало быть, географические координаты предположим (35,35), добавим этаж (5), добавим кому из семьи выдать - младшему, пусть он будет первым по счету в семье, пишем (1), и когда (15:30). 
Итого: посылка младшему должна прибыть в (35,35,5,1,15:30)",0.783,0.10714285714285765,метафора / аналогия,0.10714285714285765
"Не хочу тебя пугать, дорогой, но ты сейчас увидел пример пятимерной координаты.
Как ты, мм? Держишься?",0.798,0.2857142857142865,юмор / ирония / сарказм,0.2857142857142865
"Ну, вот видишь. Пятое измерение открыли, и ничего, нормально. 
(А Лелик, между прочим, оперирует в 12 228 мерном. И не жалуется.)",0.786,0.14285714285714235,юмор / ирония / сарказм,0.14285714285714235
"Так. Теперь дальше:
У каждого токена есть этот самый вектор ебического количества измерений. Это называется векторизация токенов.",0.828,0.6428571428571423,раскрытие темы,0.9642857142857135
"Теперь соберись, будет больно:
Во-первых, у каждого токена не просто есть свой вектор в этом многомерном пространстве — то бишь эмбединг. Он ещё и предварительный, гаврюша по ходу прожевывания смыслов его уточняет.
Во-вторых, у него ещё может быть дополнительный вектор, указывающий, где этот токен находится в общей последовательности токенов в тексте. 
Ну а как иначе. Надо ж их как-то друг за дружкой правильно выстроить, да?",0.853,0.9404761904761898,раскрытие темы,1.4107142857142847
"Если ты ещё не ушёл (а я б так и сделал на твоём месте, честно говоря) — это ещё не всё.",0.785,0.13095238095238138,юмор / ирония / сарказм,0.13095238095238138
"Этот самый эмбединг, то есть указатель, в последствии, когда гаврюша соображает, куда ты её послал, может быть не только у отдельного токена, а у любой смысловой единицы: у слова, абзаца и целого предложения.",0.83,0.6666666666666661,раскрытие темы,0.9999999999999991
"Понятно, что чем более крупная смысловая единица прицеплена к одному- единственному эмбедингу, тем этот указатель менее ценный.
— Дорогой, как тебе сегодняшний спектакль?
— Хуета.
Эмбединг как бы есть, но практического толку от него мало.",0.815,0.48809523809523725,пояснение на примере,0.6101190476190466
"Ну и кое-что надо уточнить: у каждой гаврюши своя размерность эмбедингов. 
Единая на всю модель. 
И в этом многомерном бульоне у нее внутри плавают все смыслы.",0.835,0.7261904761904763,раскрытие темы,1.0892857142857144
"Чем качественнее текст превращён в эти многомерные ёжики из указателей, тем точнее с ними можно дальше работать.",0.776,0.023809523809523725,ключевой тезис,0.03571428571428559
"Грубо (ох, простите меня, Лелик и Болик, наверно навру сейчас с три короба, но ладно), процесс ээээ.. укладки смысла фразы «закрыл замок на замок, чтобы замок не замок» в гаврюшу выглядит так:",0.796,0.26190476190476275,раскрытие темы,0.3928571428571441
"Разбиение на токены, векторизация этих токенов, добавление векторов, в какой последовательности эти токены образуют слова, превращение слов в эмбединги.",0.85,0.9047619047619051,раскрытие темы,1.3571428571428577
"Причём современные гаврюши не просто каждому «замок» дают один и тот же эмбединг, а развешивают свой для каждого следующего «замок», исходя из контекста и семантики, прости господи.",0.822,0.5714285714285712,раскрытие темы,0.8571428571428568
"Лелик, например, внутри сам развешивает эти смыслы, согласно его представлении о прекрасном.. а вот если ты решил, как я, дурак, поиграться в построение своего кастомного Дживса, (по науке это называется RAG - Retrieval-Augmented Generation, то есть генерация смыслов, дополненных внешним источником.. твоим, то есть, источником.. эээ ладно, об этом отдельный текст надо писать..) короче если ты хочешь, чтобы модель понимала смысл не от фонаря, а на базе того, что ты ей подсунешь, то для поиска смыслов твой текст надо перед превращением в набор эмбедингов еще немного обработать.",0.817,0.511904761904761,раскрытие темы,0.7678571428571415
"И тут появляется ещё одно страшное слово: чанкинг. Чанк-инг.
(Для запоминания: как челюсти клацают: чанк-чанк-чанк, смысл режется на куски)",0.808,0.4047619047619051,раскрытие темы,0.6071428571428577
"То есть если ты хочешь, чтобы один «замок» отправился в сторону архитектуры, следующий — в штуки, которые не дают дверям распахнуться, третий — опять в архитектуру (причём он ещё и тот же самый «замок», что был вначале), а последний — в свойства влажности, нужно эту фразу порезать на чанки.
Чанк — слово, чанк — следующее слово. 
Тогда каждому «замок» можно развесить по своему эмбедингу.
Там всякие про это есть хитрости, как гаврюше, в отличие от китайского студента, понять, какому «замок» — куда. Но не суть.",0.837,0.75,пояснение на примере,0.9375
"Причём, это еще не всё: можно еще и всей фразе давать целый эмбединг и тогда всю фразу в целом можно обозначить общим смыслом (как в истории про спектакль) и отправить в «поговорки», а если можно навешивать больше одного эмбединга на единицу смысла (а кто запретит) — ещё и в анекдоты про китайцев, учащих русский язык, и в «часть текста Дана, который вынес мне сегодня мозг».",0.818,0.5238095238095237,раскрытие темы,0.7857142857142856
"Фух.
Всё, выдыхай, бобёр.
До следующей серии.",0.784,0.11904761904761862,юмор / ирония / сарказм,0.11904761904761862
"Я наврал, это в ней я уже навалю про семантическую географию (бляха, я теперь могу этим термином прямо между глаз засветить кому-то) и ещё разложу, почему она бывает линейная и нелинейная.
(Ага, в следующий раз будешь сразу скроллить на котиков.)",0.798,0.2857142857142865,смена темы,0.2857142857142865
