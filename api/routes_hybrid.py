"""
–ì–∏–±—Ä–∏–¥–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback.
–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Realtime API.
"""

from fastapi import APIRouter, Body, Query, Depends, HTTPException
from typing import Optional
import logging
import asyncio

from api.models import (
    ChunkSemanticRequest, 
    ChunkSemanticResponse,
    BatchChunkSemanticRequest, 
    BatchChunkSemanticResponse
)
from services.openai_service import OpenAIService, get_openai_service
from analysis.semantic_function_hybrid import HybridSemanticAnalyzer
from config import settings

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/api/v1/hybrid", tags=["Hybrid Semantic Analysis"])


@router.post("/chunk/metrics/semantic", response_model=ChunkSemanticResponse)
async def analyze_chunk_semantic_hybrid_endpoint(
    request_data: ChunkSemanticRequest = Body(...),
    prefer_realtime: bool = Query(True, description="–ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞—Ç—å Realtime API –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏"),
    openai_service: OpenAIService = Depends(get_openai_service)
) -> ChunkSemanticResponse:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback.
    
    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Realtime API –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ (4x –±—ã—Å—Ç—Ä–µ–µ)
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–∞ REST API –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
    - 100% –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –±–ª–∞–≥–æ–¥–∞—Ä—è fallback –º–µ—Ö–∞–Ω–∏–∑–º—É
    """
    logger.info(f"[HybridAPI] –ê–Ω–∞–ª–∏–∑ —á–∞–Ω–∫–∞ {request_data.chunk_id}, prefer_realtime={prefer_realtime}")
    
    if not openai_service or not openai_service.is_available:
        return ChunkSemanticResponse(
            chunk_id=request_data.chunk_id,
            metrics={
                "semantic_function": "unavailable_api",
                "semantic_method": "hybrid",
                "semantic_error": "OpenAI service not available",
                "api_method": "none"
            }
        )
    
    try:
        # –°–æ–∑–¥–∞–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        analyzer = HybridSemanticAnalyzer(
            api_key=settings.OPENAI_API_KEY,
            prefer_realtime=prefer_realtime
        )
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞–Ω–∫
        result = await analyzer.analyze_chunk(
            chunk_id=request_data.chunk_id,
            chunk_text=request_data.chunk_text,
            topic=request_data.topic
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Ç–æ–¥–µ –≤ –º–µ—Ç—Ä–∏–∫–∏
        metrics = {
            "semantic_function": result.get("semantic_function"),
            "semantic_method": f"hybrid_{result.get('api_method', 'unknown')}",
            "semantic_error": result.get("semantic_error"),
            "api_method": result.get("api_method"),
            "api_latency": result.get("api_latency", 0)
        }
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        stats = await analyzer.get_statistics()
        api_method = result.get('api_method', 'unknown')
        logger.info(
            f"[HybridAPI] ‚úÖ –ß–∞–Ω–∫ {request_data.chunk_id} –æ–±—Ä–∞–±–æ—Ç–∞–Ω —á–µ—Ä–µ–∑ {api_method.upper()}. "
            f"–§—É–Ω–∫—Ü–∏—è: '{result.get('semantic_function', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')}'. "
            f"–í—Ä–µ–º—è: {result.get('api_latency', 0):.2f}—Å. "
            f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Realtime: {stats['realtime_successes']} —É—Å–ø–µ—Ö–æ–≤, {stats['realtime_failures']} –æ—à–∏–±–æ–∫"
        )
        
        await analyzer.close()
        
        return ChunkSemanticResponse(
            chunk_id=request_data.chunk_id,
            metrics=metrics
        )
        
    except Exception as e:
        logger.error(f"[HybridAPI] –û—à–∏–±–∫–∞ –¥–ª—è —á–∞–Ω–∫–∞ {request_data.chunk_id}: {e}", exc_info=True)
        return ChunkSemanticResponse(
            chunk_id=request_data.chunk_id,
            metrics={
                "semantic_function": "error_api_call",
                "semantic_method": "hybrid_error",
                "semantic_error": f"Hybrid endpoint error: {str(e)[:150]}",
                "api_method": "failed"
            }
        )


@router.post("/chunks/metrics/semantic-batch", response_model=BatchChunkSemanticResponse)
async def analyze_chunks_semantic_batch_hybrid_endpoint(
    request_data: BatchChunkSemanticRequest = Body(...),
    prefer_realtime: bool = Query(True, description="–ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞—Ç—å Realtime API"),
    adaptive_batching: bool = Query(True, description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –±–∞—Ç—á–∏–Ω–≥–∞"),
    openai_service: OpenAIService = Depends(get_openai_service)
) -> BatchChunkSemanticResponse:
    """
    –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º.
    
    –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è:
    - –î–ª—è –º–∞–ª—ã—Ö –æ–±—ä–µ–º–æ–≤ (<10 —á–∞–Ω–∫–æ–≤): –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç Realtime API
    - –î–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤: –ø–µ—Ä–≤—ã–µ 25% —á–µ—Ä–µ–∑ Realtime, –æ—Å—Ç–∞–ª—å–Ω—ã–µ —á–µ—Ä–µ–∑ REST
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
    - –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
    """
    logger.info(
        f"[HybridBatchAPI] üì¶ –ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ {len(request_data.chunks)} —á–∞–Ω–∫–æ–≤, "
        f"prefer_realtime={prefer_realtime}, adaptive={adaptive_batching}"
    )
    
    if not openai_service or not openai_service.is_available:
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫–∏ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
        failed_results = []
        all_failed = []
        
        for chunk in request_data.chunks:
            chunk_id = chunk.get("id", "unknown")
            failed_results.append(ChunkSemanticResponse(
                chunk_id=chunk_id,
                metrics={
                    "semantic_function": "unavailable_api",
                    "semantic_method": "hybrid",
                    "semantic_error": "OpenAI service not available",
                    "api_method": "none"
                }
            ))
            all_failed.append(chunk_id)
        
        return BatchChunkSemanticResponse(results=failed_results, failed=all_failed)
    
    try:
        # –°–æ–∑–¥–∞–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        analyzer = HybridSemanticAnalyzer(
            api_key=settings.OPENAI_API_KEY,
            prefer_realtime=prefer_realtime
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
        if len(request_data.chunks) > 10:
            # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –ø–∞—É–∑–∞–º–∏
            logger.info(f"[HybridBatchAPI] üêå –ë–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç ({len(request_data.chunks)} —á–∞–Ω–∫–æ–≤) - –¥–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫–∏")
            results = []
            for i, chunk in enumerate(request_data.chunks):
                result = await analyzer.analyze_chunk(
                    chunk_id=chunk["id"],
                    chunk_text=chunk["text"],
                    topic=request_data.topic
                )
                results.append(result)
                
                # –ü–∞—É–∑–∞ –∫–∞–∂–¥—ã–µ 5 —á–∞–Ω–∫–æ–≤ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è rate limit
                if (i + 1) % 5 == 0 and i < len(request_data.chunks) - 1:
                    logger.info(f"[HybridBatchAPI] ‚è∏Ô∏è –ü–∞—É–∑–∞ –ø–æ—Å–ª–µ {i + 1} —á–∞–Ω–∫–æ–≤ (2 —Å–µ–∫)")
                    await asyncio.sleep(2.0)
        else:
            # –î–ª—è –º–∞–ª—ã—Ö –æ–±—ä–µ–º–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –±–∞—Ç—á–∏–Ω–≥
            results = await analyzer.analyze_batch(
                chunks=request_data.chunks,
                topic=request_data.topic,
                max_concurrent=request_data.max_parallel or 5,
                adaptive_batching=adaptive_batching
            )
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞
        response_results = []
        failed = []
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –º–µ—Ç–æ–¥–∞–º
        method_stats = {"realtime": 0, "rest": 0, "failed": 0}
        
        for result in results:
            chunk_id = result["chunk_id"]
            api_method = result.get("api_method", "unknown")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            if api_method == "realtime":
                method_stats["realtime"] += 1
            elif api_method == "rest":
                method_stats["rest"] += 1
            else:
                method_stats["failed"] += 1
                failed.append(chunk_id)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
            metrics = {
                "semantic_function": result.get("semantic_function"),
                "semantic_method": f"hybrid_{api_method}",
                "semantic_error": result.get("semantic_error"),
                "api_method": api_method,
                "api_latency": result.get("api_latency", 0)
            }
            
            response_results.append(ChunkSemanticResponse(
                chunk_id=chunk_id,
                metrics=metrics
            ))
        
        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = await analyzer.get_statistics()
        logger.info(
            f"[HybridBatchAPI] ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω –∞–Ω–∞–ª–∏–∑ {len(request_data.chunks)} —á–∞–Ω–∫–æ–≤. "
            f"–†–µ–∑—É–ª—å—Ç–∞—Ç: Realtime={method_stats['realtime']}, REST={method_stats['rest']}, –û—à–∏–±–∫–∏={method_stats['failed']}. "
            f"Realtime –¥–æ—Å—Ç—É–ø–µ–Ω: {'–î–ê' if stats['realtime_available'] else '–ù–ï–¢'}. "
            f"–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Realtime: {stats['realtime_successes']} —É—Å–ø–µ—Ö–æ–≤, {stats['realtime_failures']} –æ—à–∏–±–æ–∫"
        )
        
        await analyzer.close()
        
        return BatchChunkSemanticResponse(results=response_results, failed=failed)
        
    except Exception as e:
        logger.error(f"[HybridBatchAPI] –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫–∏ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
        failed_results = []
        all_failed = []
        
        for chunk in request_data.chunks:
            chunk_id = chunk.get("id", "unknown")
            failed_results.append(ChunkSemanticResponse(
                chunk_id=chunk_id,
                metrics={
                    "semantic_function": "error_api_call",
                    "semantic_method": "hybrid_error",
                    "semantic_error": f"Hybrid batch error: {str(e)[:100]}",
                    "api_method": "failed"
                }
            ))
            all_failed.append(chunk_id)
        
        return BatchChunkSemanticResponse(results=failed_results, failed=all_failed)


@router.get("/stats", tags=["Hybrid Semantic Analysis"])
async def get_hybrid_stats(
    openai_service: OpenAIService = Depends(get_openai_service)
) -> dict:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ API.
    
    –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç:
    - –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Realtime API
    - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω—ã—Ö/–Ω–µ—É—Å–ø–µ—à–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤
    - –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ—Å—Å–∏–∏
    """
    if not openai_service or not openai_service.is_available:
        return {
            "status": "unavailable",
            "message": "OpenAI service not configured"
        }
    
    try:
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        analyzer = HybridSemanticAnalyzer(api_key=settings.OPENAI_API_KEY)
        stats = await analyzer.get_statistics()
        await analyzer.close()
        
        return {
            "status": "available",
            "realtime_api": {
                "available": stats["realtime_available"],
                "failures": stats["realtime_failures"],
                "successes": stats["realtime_successes"],
                "last_failure": stats["last_failure"]
            },
            "recommendation": (
                "Realtime API —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ" 
                if stats["realtime_available"] 
                else "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback –Ω–∞ REST API"
            )
        }
        
    except Exception as e:
        logger.error(f"[HybridStats] –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        return {
            "status": "error",
            "message": str(e)
        } 