# Text Quality Analyzer — Документация проекта

## Оглавление
1. [Общее описание и цели](#общее-описание-и-цели)
2. [Архитектура и компоненты](#архитектура-и-компоненты)
3. [Бэкенд (FastAPI)](#бэкенд-fastapi)
    - [Задачи и принципы](#задачи-и-принципы)
    - [Установка и запуск](#установка-и-запуск-бэкенда)
    - [Структура кода](#структура-кода)
    - [Модули анализа](#модули-анализа)
    - [API и эндпоинты](#api-и-эндпоинты)
    - [Безопасность и деплой](#безопасность-и-деплой)
4. [Фронтенд (React CardView)](#фронтенд-react-cardview)
    - [Возможности и режимы](#возможности-и-режимы)
    - [Установка и запуск](#установка-и-запуск-фронтенда)
    - [Архитектура и компоненты](#архитектура-и-компоненты-фронтенда)
    - [Работа с API и сценарии](#работа-с-api-и-сценарии)
    - [Кастомизация и расширение](#кастомизация-и-расширение)

---

## Общее описание и цели

**Text Quality Analyzer** — это система для интерактивного анализа и визуализации текстов по метрикам сложности, сигнальности (соответствия теме) и семантической функции абзацев. Проект предназначен для авторов, редакторов и исследователей, которым важно:
- Повышать читаемость и смысловую плотность текстов
- Быстро выявлять отклонения от темы и «шумовые» фрагменты
- Классифицировать смысловые блоки по их роли (тезис, пример, метафора, шутка и т.д.)
- Получать наглядную тепловую карту и работать с текстом в интерактивном режиме

**Ключевые задачи:**
- Автоматическая разбивка текста на абзацы
- Оценка каждого блока по сложности, сигнал/шум, семантической функции
- Визуализация результатов в виде карточек и тепловой карты
- Гибкая архитектура для расширения и интеграции

---

## Архитектура и компоненты

Система построена по современной клиент-серверной архитектуре:

- **Бэкенд:** Python, FastAPI, Pandas, ML-модули анализа (запускается в Docker)
- **Фронтенд:** React, TypeScript, Vite, dnd-kit, Framer Motion (запускается локально)

### Требования к системе
- **Для запуска бэкенда:**
  - Docker и Docker Compose
  - 4 ГБ ОЗУ минимум (8 ГБ рекомендуется)
  - 10 ГБ свободного места на диске
  - Доступ к интернету для загрузки контейнеров и доступа к OpenAI API

- **Для запуска фронтенда:**
  - Современный браузер (Chrome, Firefox, Edge)
  - Node.js 18+ и npm/yarn
  - 2 ГБ ОЗУ минимум

### Схема взаимодействия

1. Пользователь загружает текст и тему через веб-интерфейс
2. Фронтенд отправляет данные на FastAPI-бэкенд
3. Бэкенд анализирует текст, возвращает результаты (JSON)
4. Фронтенд визуализирует результаты, позволяет интерактивно работать с абзацами
5. Все изменения (редактирование, удаление, drag-and-drop) синхронизируются с сервером

---

## Бэкенд (FastAPI)

### Задачи и принципы
- Приём текста и темы для анализа
- Оркестрация модулей анализа (читаемость, сигнал/шум, семантика)
- Хранение и обновление сессий анализа
- Инкрементальное обновление при изменении абзацев
- Экспорт результатов (CSV/JSON)
- Безопасность, валидация, логирование
- Использование внешней LLM (OpenAI API) для семантического анализа

### Установка и запуск бэкенда

**Требования:**
- Docker и Docker Compose
- OpenAI API ключ (опционально)

**Установка и запуск через Docker:**
```bash
# Клонируйте репозиторий
$ git clone ...
$ cd text_quality_analyzer

# Создайте .env файл с необходимыми переменными окружения
$ cp example.env .env
# Отредактируйте .env файл, добавив ваш OPENAI_API_KEY и другие необходимые параметры

# Запустите бэкенд через Docker Compose
$ docker-compose up -d

# Для просмотра логов
$ docker-compose logs -f
```

**Переменные окружения:**
В проекте используется файл example.env с примером всех доступных настроек. Основные параметры:
```
OPENAI_API_KEY=sk-...   # Ключ API OpenAI для семантического анализа
REDIS_URL=redis://redis:6379/0  # URL для подключения к Redis
CORS_ORIGINS=http://localhost:5173  # Разрешенные источники для CORS
VITE_API_URL=http://localhost:8000  # URL API для фронтенда
DEBUG=False  # Режим отладки
ENABLE_SEMANTIC_ANALYSIS=True  # Включение семантического анализа
LOG_LEVEL=INFO  # Уровень логирования
```

**Доступ к API и документации:**
- API доступен по адресу: http://localhost:8000
- Swagger UI: http://localhost:8000/docs

### Структура кода
```
text-analyzer/
├── api/
│   ├── __init__.py
│   ├── models.py        # Pydantic модели
│   ├── routes.py        # API-маршруты
│   └── orchestrator.py  # Оркестратор анализа
├── analysis/
│   ├── readability.py
│   ├── signal_strength.py
│   └── semantic_function.py
├── services/
│   ├── session_store.py
│   ├── embedding_service.py
│   └── export_service.py
├── utils/
│   ├── text_processing.py
│   └── async_helpers.py
├── config.py
├── logging_config.py
├── main.py
└── requirements.txt
```

### Модули анализа

#### 1. Модуль оценки читаемости (readability.py)
- **Метрики:**
  - LIX: `LIX = (кол-во слов / кол-во предложений) + (100 * кол-во длинных слов (>6 символов) / кол-во слов)`
  - SMOG: `SMOG = 1.0430 * sqrt(кол-во сложных слов * (30 / кол-во предложений)) + 3.1291`
- **Пример кода:**
```python
import re
import math

def lix(text):
    sentences = re.split(r'[.!?]', text)
    words = re.findall(r'\w+', text)
    long_words = [w for w in words if len(w) > 6]
    return len(words) / max(1, len(sentences)) + 100 * len(long_words) / max(1, len(words))

def smog(text):
    sentences = re.split(r'[.!?]', text)
    complex_words = [w for w in re.findall(r'\w+', text) if count_syllables(w) >= 3]
    if len(sentences) < 3:
        return None
    return 1.0430 * math.sqrt(len(complex_words) * (30 / len(sentences))) + 3.1291
```
- **Нормализация:** значения LIX и SMOG приводятся к шкале 0–1 для сравнения между абзацами.

#### 2. Модуль оценки Сигнал/Шум (signal_strength.py)
- **Используемая модель:** `intfloat/multilingual-e5-large` (локально, через HuggingFace Transformers)
- **Принцип:**
  - Для темы и каждого абзаца вычисляется эмбеддинг (vector)
  - Сигнал/Шум = косинусная близость между эмбеддингом темы и эмбеддингом абзаца
- **Пример кода:**
```python
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('intfloat/multilingual-e5-large')
def signal_strength(paragraph, topic):
    emb_par = model.encode(paragraph, convert_to_tensor=True)
    emb_topic = model.encode(topic, convert_to_tensor=True)
    return float(util.pytorch_cos_sim(emb_par, emb_topic))
```
- **Интерпретация:**
  - 1.0 — абзац максимально по теме
  - 0.0 — абзац не по теме

#### 3. Модуль семантической функции (semantic_function.py)
- **Реализация:** запрос к внешней LLM через OpenAI API (GPT-4.1)
- **Принцип:**
  - Все абзацы отправляются в одном запросе
  - Модель анализирует текст и возвращает семантическую функцию для каждого абзаца
  - Система классифицирует абзацы по таким категориям, как "ключевой тезис", "пример", "метафора", "шутка" и т.д.

#### 4. Экспериментальная поддержка OpenAI Realtime API (semantic_function_realtime.py)
- **Статус:** Экспериментальная функция, в разработке
- **Описание:** Альтернативная реализация семантического анализа через WebSocket-based Realtime API
- **Преимущества:**
  - Низкая латентность (в 4.4x быстрее на малых объемах)
  - Поддержка стриминга
  - Единая сессия для всех запросов
- **Ограничения:**
  - Требует temperature >= 0.6
  - Проблемы с масштабированием на больших объемах
  - Не рекомендуется для production
- **Документация:** [docs/realtime-api-research.md](docs/realtime-api-research.md)

### API и эндпоинты
| Эндпоинт | Метод | Описание |
|---|---|---|
| `/api/analyze` | POST | Полный анализ текста: `{text, topic}` → JSON сессии |
| `/api/update-paragraph` | POST | Обновление абзаца: `{session_id, paragraph_id, text}` |
| `/api/paragraph/update-text-and-restructure` | POST | Обновление текста абзаца с возможным разделением или удалением: `{session_id, paragraph_id, text}` → полный анализ |
| `/api/analysis/{session_id}` | GET | Получение анализа по session_id |
| `/api/analysis/{session_id}/refresh-semantics` | POST | Пересчет семантического анализа для всей сессии |
| `/api/export/{session_id}` | GET | Экспорт в CSV/JSON |
| `/api/merge-paragraphs` | POST | Объединение двух абзацев: `{session_id, paragraph_id_1, paragraph_id_2}` |
| `/api/split-paragraph` | POST | Разделение абзаца: `{session_id, paragraph_id, split_position}` |
| `/api/reorder-paragraphs` | POST | Изменение порядка абзацев: `{session_id, new_order: number[]}` |
| `/api/update-topic` | POST | Обновление темы: `{session_id, topic}` |
| `/api/paragraph/{session_id}/{paragraph_id}` | DELETE | Удаление абзаца |
| `/health` | GET | Проверка статуса API |

**Пример структуры ответа:**
```json
{
  "metadata": {
    "session_id": "...",
    "topic": "...",
    "analysis_timestamp": "...",
    "paragraph_count": 15,
    "avg_complexity": 0.68,
    "avg_signal_strength": 0.72
  },
  "paragraphs": [
    {
      "id": 0,
      "text": "...",
      "metrics": {
        "complexity": 0.75,
        "lix": 43.2,
        "smog": 12.5,
        "signal_strength": 0.91,
        "semantic_function": "ключевой тезис",
        "semantic_method": "api"
      }
    }
    // ...
  ]
}
```

#### Безопасность и деплой
- Валидация входных данных (Pydantic)
- Rate limiting (Redis)
- Логирование (RotatingFileHandler)
- Docker/Docker Compose для продакшена
- Swagger/OpenAPI-документация

---

## Фронтенд (React CardView)

### Возможности и режимы
- **Редактор:** Ввод и редактирование всего текста и темы, отправка на анализ
- **Карточки:** Визуализация абзацев, интерактивная работа с каждым блоком
- **Возможности:**
  - Переключение между режимами (редактор/карточки)
  - Загрузка и редактирование текста, темы
  - Просмотр и настройка тепловой карты (цвета, диапазоны)
  - Панель управления (шрифт, цвета, фильтры)
  - Поиск и фильтрация по тексту и семантике
  - Drag-and-drop карточек (dnd-kit)
  - Редактирование, удаление, слияние, разделение абзацев
  - Анимации (Framer Motion)
  - Индикация загрузки, ошибок, состояния

### Установка и запуск фронтенда

**Требования:**
- Node.js 18+
- npm или yarn

**Установка:**
```bash
$ cd frontend/my-card-view-app
$ npm install
```

**Запуск:**
```bash
$ npm run dev
```

Фронтенд будет доступен по адресу: http://localhost:5173

**Сборка для продакшена:**
```bash
$ npm run build
```

### Архитектура и компоненты фронтенда
```
frontend/my-card-view-app/
├── public/
│   ├── card_view_data.json
│   └── config.json
├── src/
│   ├── api/
│   │   └── index.ts     # API-клиент
│   ├── components/
│   │   ├── CardView/
│   │   │   ├── Card.tsx
│   │   │   ├── CardList.tsx
│   │   │   ├── HeatMap.tsx
│   │   │   ├── Controls.tsx
│   │   │   ├── SemanticIcon/
│   │   │   └── types.ts
│   │   ├── Editor/
│   │   └── shared/
│   ├── assets/
│   │   └── icons/
│   ├── App.tsx
│   └── main.tsx
└── ...
```

### Работа с API и сценарии
- Все операции (анализ, обновление, экспорт) через HTTP API по адресу `http://localhost:8000`
- URL API настраивается в файле `frontend/my-card-view-app/src/api/index.ts` 
- Поддержка сессий анализа (session_id)
- Инкрементальное обновление при изменении абзацев
- Пример работы:
  1. Загрузка текста → анализ → работа с карточками
  2. Редактирование абзаца → мгновенное обновление метрик
  3. Перетаскивание карточек → изменение порядка
  4. Настройка цветов, шрифта, фильтров → мгновенное обновление UI

### Кастомизация и расширение
- Все параметры визуализации (цвета, шрифт, фильтры) настраиваются через UI
- Легко расширяется за счёт модульной архитектуры (добавление новых метрик, фильтров, режимов)
- Поддержка анимаций и drag-and-drop (dnd-kit, Framer Motion)
- Возможность интеграции с внешними системами через API

---

## Типичные проблемы и их решения

### Проблемы с запуском

| Проблема | Возможная причина | Решение |
|---|---|---|
| Ошибка при запуске Docker: `permission denied` | Недостаточно прав для работы с Docker | Добавьте пользователя в группу docker: `sudo usermod -aG docker $USER`, затем перезагрузите систему |
| Фронтенд не может подключиться к API | Неправильная конфигурация CORS | Проверьте CORS_ORIGINS в .env файле, убедитесь, что URL фронтенда там указан |
| Ошибка `OPENAI_API_KEY is invalid` | Отсутствует или неверный ключ API | Проверьте ключ OpenAI API в .env файле |
| Ошибка памяти при анализе больших текстов | Недостаточно RAM | Увеличьте выделенную память для Docker в настройках |

### Проблемы при анализе

| Проблема | Возможная причина | Решение |
|---|---|---|
| Низкое качество семантического анализа | Проблемы с OpenAI API | Проверьте логи на ошибки API и лимиты запросов |
| Анализ занимает слишком много времени | Большой объем текста или медленное соединение | Разделите анализ на меньшие части |
| Ошибки при экспорте | Проблемы с форматированием или правами доступа | Проверьте наличие директории `exports` и права записи |

## Контакты и развитие
- [ ] Добавить раздел FAQ и troubleshooting
- [ ] Описать расширение API и интеграцию с внешними системами
- [ ] Примеры деплоя (Docker, Nginx, облако)

---

© 2024, Text Quality Analyzer Project 