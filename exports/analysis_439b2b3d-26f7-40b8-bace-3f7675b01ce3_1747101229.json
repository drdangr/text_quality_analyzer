{
  "metadata": {
    "session_id": "439b2b3d-26f7-40b8-bace-3f7675b01ce3",
    "topic": "Основные концепции обработки естественного языка",
    "export_timestamp_unix": 1747101229,
    "export_timestamp_iso": "2025-05-13T01:53:49+00:00",
    "paragraph_count": 3
  },
  "paragraphs": [
    {
      "id": 0,
      "text": "Это совершенно новый текст для первого абзаца, который мы обновляем инкрементально.",
      "metrics": {
        "lix": 56.455,
        "smog": 17.122,
        "complexity": 0.706,
        "signal_strength": 0.794,
        "semantic_function": "шум",
        "semantic_method": "api",
        "semantic_error": null
      }
    },
    {
      "id": 1,
      "text": "Токенизация это процесс разбиения текста на мелкие части, называемые токенами. Это помогает в анализе.",
      "metrics": {
        "lix": 60.346,
        "smog": 13.817,
        "complexity": 0.754,
        "signal_strength": 0.8,
        "semantic_function": "раскрытие темы",
        "semantic_method": "api",
        "semantic_error": null
      }
    },
    {
      "id": 2,
      "text": "Чанкинг делит большие тексты на части поменьше. Это нужно для моделей с лимитом токенов.",
      "metrics": {
        "lix": 52.654,
        "smog": 12.162,
        "complexity": 0.658,
        "signal_strength": 0.802,
        "semantic_function": "раскрытие темы",
        "semantic_method": "api",
        "semantic_error": null
      }
    }
  ]
}