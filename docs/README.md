# Text Quality Analyzer — Документация проекта

## Оглавление
1. [Общее описание и цели](#общее-описание-и-цели)
2. [Архитектура и компоненты](#архитектура-и-компоненты)
3. [Бэкенд (FastAPI)](#бэкенд-fastapi)
    - [Задачи и принципы](#задачи-и-принципы)
    - [Установка и запуск](#установка-и-запуск-бэкенда)
    - [Структура кода](#структура-кода)
    - [Модули анализа](#модули-анализа)
    - [API и эндпоинты](#api-и-эндпоинты)
    - [Безопасность и деплой](#безопасность-и-деплой)
4. [Фронтенд (React CardView)](#фронтенд-react-cardview)
    - [Возможности и режимы](#возможности-и-режимы)
    - [Установка и запуск](#установка-и-запуск-фронтенда)
    - [Архитектура и компоненты](#архитектура-и-компоненты-фронтенда)
    - [Работа с API и сценарии](#работа-с-api-и-сценарии)
    - [Кастомизация и расширение](#кастомизация-и-расширение)

---

## Общее описание и цели

**Text Quality Analyzer** — это система для интерактивного анализа и визуализации текстов по метрикам сложности, сигнальности (соответствия теме) и семантической функции абзацев. Проект предназначен для авторов, редакторов и исследователей, которым важно:
- Повышать читаемость и смысловую плотность текстов
- Быстро выявлять отклонения от темы и «шумовые» фрагменты
- Классифицировать смысловые блоки по их роли (тезис, пример, метафора, шутка и т.д.)
- Получать наглядную тепловую карту и работать с текстом в интерактивном режиме

**Ключевые задачи:**
- Автоматическая разбивка текста на абзацы
- Оценка каждого блока по сложности, сигнал/шум, семантической функции
- Визуализация результатов в виде карточек и тепловой карты
- Гибкая архитектура для расширения и интеграции

---

## Архитектура и компоненты

Система построена по современной клиент-серверной архитектуре:

- **Бэкенд:** Python, FastAPI, Pandas, ML-модули анализа
- **Фронтенд:** React, TypeScript, Vite, dnd-kit, Framer Motion

### Схема взаимодействия

1. Пользователь загружает текст и тему через веб-интерфейс
2. Фронтенд отправляет данные на FastAPI-бэкенд
3. Бэкенд анализирует текст, возвращает результаты (JSON)
4. Фронтенд визуализирует результаты, позволяет интерактивно работать с абзацами
5. Все изменения (редактирование, удаление, drag-and-drop) синхронизируются с сервером

---

## Бэкенд (FastAPI)

### Задачи и принципы
- Приём текста и темы для анализа
- Оркестрация модулей анализа (читаемость, сигнал/шум, семантика)
- Хранение и обновление сессий анализа
- Инкрементальное обновление при изменении абзацев
- Экспорт результатов (CSV/JSON)
- Безопасность, валидация, логирование
- Использование внешней LLM (OpenAI API) для семантического анализа (fallback на локальную модель)

### Установка и запуск бэкенда

**Требования:**
- Python 3.10+
- pip
- (Рекомендуется) venv/virtualenv

**Установка:**
```bash
# Клонируйте репозиторий
$ git clone ...
$ cd text_quality_analyzer/backend

# Создайте и активируйте виртуальное окружение
$ python -m venv venv
$ source venv/bin/activate  # или venv\Scripts\activate на Windows

# Установите зависимости
$ pip install -r requirements.txt
```

**Переменные окружения (.env):**
```
OPENAI_API_KEY=sk-...   # Для семантического анализа через OpenAI
REDIS_URL=redis://localhost:6379/0
CORS_ORIGINS=http://localhost:3000,http://localhost:5173
```

**Запуск:**
```bash
$ uvicorn main:app --reload
```

**Swagger UI:** http://localhost:8000/docs

### Структура кода
```
text-analyzer/
├── api/
│   ├── __init__.py
│   ├── models.py        # Pydantic модели
│   ├── routes.py        # API-маршруты
│   └── orchestrator.py  # Оркестратор анализа
├── analysis/
│   ├── readability.py
│   ├── signal_strength.py
│   └── semantic_function.py
├── services/
│   ├── session_store.py
│   ├── embedding_service.py
│   └── export_service.py
├── utils/
│   ├── text_processing.py
│   └── async_helpers.py
├── config.py
├── logging_config.py
├── main.py
└── requirements.txt
```

### Модули анализа

#### 1. Модуль оценки читаемости (readability.py)
- **Метрики:**
  - LIX: `LIX = (кол-во слов / кол-во предложений) + (100 * кол-во длинных слов (>6 символов) / кол-во слов)`
  - SMOG: `SMOG = 1.0430 * sqrt(кол-во сложных слов * (30 / кол-во предложений)) + 3.1291`
- **Пример кода:**
```python
import re
import math

def lix(text):
    sentences = re.split(r'[.!?]', text)
    words = re.findall(r'\w+', text)
    long_words = [w for w in words if len(w) > 6]
    return len(words) / max(1, len(sentences)) + 100 * len(long_words) / max(1, len(words))

def smog(text):
    sentences = re.split(r'[.!?]', text)
    complex_words = [w for w in re.findall(r'\w+', text) if count_syllables(w) >= 3]
    if len(sentences) < 3:
        return None
    return 1.0430 * math.sqrt(len(complex_words) * (30 / len(sentences))) + 3.1291
```
- **Нормализация:** значения LIX и SMOG приводятся к шкале 0–1 для сравнения между абзацами.

#### 2. Модуль оценки Сигнал/Шум (signal_strength.py)
- **Используемая модель:** `intfloat/multilingual-e5-large` (локально, через HuggingFace Transformers)
- **Принцип:**
  - Для темы и каждого абзаца вычисляется эмбеддинг (vector)
  - Сигнал/Шум = косинусная близость между эмбеддингом темы и эмбеддингом абзаца
- **Пример кода:**
```python
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('intfloat/multilingual-e5-large')
def signal_strength(paragraph, topic):
    emb_par = model.encode(paragraph, convert_to_tensor=True)
    emb_topic = model.encode(topic, convert_to_tensor=True)
    return float(util.pytorch_cos_sim(emb_par, emb_topic))
```
- **Интерпретация:**
  - 1.0 — абзац максимально по теме
  - 0.0 — абзац не по теме

#### 3. Модуль семантической функции (semantic_function.py)
- **Основной режим:** запрос к внешней LLM через OpenAI API (GPT-4.1)
- **Fallback:** zero-shot классификация локальной моделью (MoritzLaurer/mDeBERTa-v3-base-mnli-xnli)
- **Принцип:**
  - Все абзацы отправляются в одном запросе (если возможно)
  - Модель возвращает список меток (тезис, пример, метафора, и т.д.)
  - Если API недоступен — используется локальная модель

### API и эндпоинты
| Эндпоинт | Метод | Описание |
|---|---|---|
| `/api/analyze` | POST | Полный анализ текста: `{text, topic}` → JSON сессии |
| `/api/update-paragraph` | POST | Обновление абзаца: `{session_id, paragraph_id, text}` |
| `/api/analysis/{session_id}` | GET | Получение анализа по session_id |
| `/api/export/{session_id}` | GET | Экспорт в CSV/JSON |
| `/health` | GET | Проверка статуса |

**Пример структуры ответа:**
```json
{
  "metadata": {
    "session_id": "...",
    "topic": "...",
    "analysis_timestamp": "...",
    "paragraph_count": 15,
    "avg_complexity": 0.68,
    "avg_signal_strength": 0.72
  },
  "paragraphs": [
    {
      "id": 0,
      "text": "...",
      "metrics": {
        "complexity": 0.75,
        "lix": 43.2,
        "smog": 12.5,
        "signal_strength": 0.91,
        "semantic_function": "ключевой тезис",
        "semantic_method": "api"
      }
    }
    // ...
  ]
}
```

#### Безопасность и деплой
- Валидация входных данных (Pydantic)
- Rate limiting (Redis)
- Логирование (RotatingFileHandler)
- Docker/Docker Compose для продакшена
- Swagger/OpenAPI-документация

---

## Фронтенд (React CardView)

### Возможности и режимы
- **Редактор:** Ввод и редактирование всего текста и темы, отправка на анализ
- **Карточки:** Визуализация абзацев, интерактивная работа с каждым блоком
- **Возможности:**
  - Переключение между режимами (редактор/карточки)
  - Загрузка и редактирование текста, темы
  - Просмотр и настройка тепловой карты (цвета, диапазоны)
  - Панель управления (шрифт, цвета, фильтры)
  - Поиск и фильтрация по тексту и семантике
  - Drag-and-drop карточек (dnd-kit)
  - Редактирование, удаление, слияние, разделение абзацев
  - Анимации (Framer Motion)
  - Индикация загрузки, ошибок, состояния

### Установка и запуск фронтенда

**Требования:**
- Node.js 18+
- npm или yarn

**Установка:**
```bash
$ cd frontend/my-card-view-app
$ npm install
```

**Запуск:**
```bash
$ npm run dev
```

**Сборка для продакшена:**
```bash
$ npm run build
```

### Архитектура и компоненты фронтенда
```
frontend/my-card-view-app/
├── public/
│   ├── card_view_data.json
│   └── config.json
├── src/
│   ├── components/
│   │   └── CardView/
│   │       ├── Card.tsx
│   │       ├── CardList.tsx
│   │       └── types.ts
│   ├── App.tsx
│   └── main.tsx
└── ...
```

### Работа с API и сценарии
- Все операции (анализ, обновление, экспорт) через HTTP API
- Поддержка сессий анализа (session_id)
- Инкрементальное обновление при изменении абзацев
- Примеры запросов см. в разделе API
- Пример работы:
  1. Загрузка текста → анализ → работа с карточками
  2. Редактирование абзаца → мгновенное обновление метрик
  3. Перетаскивание карточек → изменение порядка
  4. Настройка цветов, шрифта, фильтров → мгновенное обновление UI

### Кастомизация и расширение
- Все параметры визуализации (цвета, шрифт, фильтры) настраиваются через UI
- Легко расширяется за счёт модульной архитектуры (добавление новых метрик, фильтров, режимов)
- Поддержка анимаций и drag-and-drop (dnd-kit, Framer Motion)
- Возможность интеграции с внешними системами через API

---

## Контакты и развитие
- [ ] Добавить раздел FAQ и troubleshooting
- [ ] Описать расширение API и интеграцию с внешними системами
- [ ] Примеры деплоя (Docker, Nginx, облако)

---

© 2024, Text Quality Analyzer Project 