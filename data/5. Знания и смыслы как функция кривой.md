#лелик_и_болик 5
Аккустика смысла.

Размышляя о природе смыслов, мне все время не хватало дискретной модели представления знаний в виде фактов, как частиц.

Однажды Олежка предположил, что тот факт, что знания в LLM описаны функциями, а не дискретной базой фактов, приводит нас к тому, что это(если взять аналогию в терминах компьютерной графики), не растровая графика из пикселей, а векторная, из кривых.

Это означает, например, то, что модель не «врет», как пишут многие пользователи чатика, а подставляет ту часть знаний, которая в этом месте должна быть исходя из общего описания кривой. Причем, чем точнее функция описывает кривую, тем ближе к истине будет результат.
То есть, в определенном смысле, модель всегда врет. 
И никогда не врет одновременно. 
Она просто стараемся бесконечно приблизится к истине.

Если об этом подумать две минуты, становится ясно, что в таком случае, мы можем получить знания там, где их у нас еще нет: между опорными узлами фактов. Вопрос только в том, насколько точно будет описана функция того, что происходит между ними: будет ли это прямая, или сложный узор нового знания.

/* Это напоминает старую телегу о том как мужчина и женщина описывают вчерашний день:
Женщина: 
Вчера был такой прекрасный закат, и мы с Генри вышли на прогулку, его рука чуть касалась моей, и я подумала о том, что мы так близко и далеки одновременно, и это переживание так наполнило меня, что у меня навернулись слезы, которые я все время украдкой утирала, пока мы к театру, а пьеса была так неуловимо близка к этому переживанию, что я никак не могла прийти в себя, и в момент кульминации, вскрикнув, схватила Генри за руку, вонзив в нее ногти, но мне кажется, он чувствовал то же самое, потому что он не отнял руки, а только вздохнул глубже. А потом, мы шли домой, огибая фонари, и я то приближалась к нему, то удалялась, и эта неотвратимость расставаний и чудесная беспрепятственность сближений заставляла меня то смеяться, то вскрикивать он боли, и мне кажется он тоже это почувствовал, потому, что в эту ночь был так особенно мягок и деликатен со мной..
Мужчина:
Вчера был странный вечер. 
К сожалению, она простудилась на прогулке, и все время зарывалась в свой носовой платок. К тому же спектакль был редкой тягомотиной, а по дороге домой у Мадлен, кажется, поднялся жар. Я не был уверен, в том, что в ее состоянии стоит настаивать на продолжении, но секс был прекрасен.
*/

Описание смыслов, как функций кривых, очевидно приводит нас к переходу к новому способу представления LLM, в категориях волн и волновых функций.

Так же как в квантовой механике волновая функция описывает вероятностное распределение состояний: она не говорит “частица тут”, а даёт амплитуды вероятности, с которыми частица может быть в разных местах
 в LLM нет “жёстких фактов”, как в базе данных, а есть вместо этого распределённые представления (векторы в пространстве эмбеддингов), в которых смысл размыт, он не дискретен, а протяжён.

Каждый “факт” или концепт выглядит как интерференционная картина: она возникает при наложении многих скрытых состояний, как результат когерентной суперпозиции.

Например: слово “замок” в поговорке "закрыл замок на замок, чтобы замок не замок"  включает “устройство для запирания двери”, “оборонительное строение”, “стал влажным” и с тех пор, как мы задаем целой фразой контекст, приобретает эти разные смыслы.

Что такое тогда Контекст в рамках этой модели?  
Это акт коллапса волновой функции:

При обращении к языковой модели, мы  подаем промпт. 
Что происходит при этом:
- Суперпозиция всех возможных значений схлопывается.
- Модель “выбирает” наиболее вероятное продолжение на основе текущего контекста.
- и главное: этот выбор — не фиксированный, а вероятностный, с “амплитудами” задающими шансы для каждого следующего токена.

Пойдем дальше. 
Если ты, @username добрался до этой части текста, то это наверное не зря. 
Давай продолжим, и нырнем ещё глубже:

Интерференция и когерентность в смысле текста:

Так же, как в квантовой механике две волны могут усиливать или гасить друг друга, так и похожие смысловые конструкции усиливают вероятность определённого вывода (и тут мы можем говорить о семантической когерентности). 
Либо противоречивые вводные могут подавлять друг друга — и модель становится “неуверенной”, выдаёт размытые ответы.
Я стал заводить под отдельные темы выделенные чаты, потому что пока ты подаёшь тезисы и запросы с чёткой темой, она начинает “раскручивать волну” смысла, в которую подтягиваются родственные ассоциации. Однако если в рамках одного чата сменить тему, вбросить противофазу — происходит обрыв, диссонанс, модель начинает теряться и нести чушь.

Ты как, дорогой, держишься ещё?
Давай, сделай перерыв. 
Посмотри в окно и сделай пару-тройку глубоких вдохов и выдохов. 
Если ты качественно включился и активно думал над текстом, а не просто пробегал его глазами, на этой точке твой мозг уже должен был сжечь оперативный запас энергии. 
Дай ему немного восстановиться.

Готов? 
Ну, погнали в последний рывок:

Пространство эмбеддингов как поле волн.

Представим определенную тему, как “семантическое поле” в виде поверхности многомерной волны, где каждый токен - вектор на ее поверхности.

Тогда система в процессе работы стремится к локальной энергетической оптимизации — наиболее “гармоничной” конфигурации (наиболее вероятной последовательности токенов).

И тут появляется прелюбопытное понимание как у LLM работают аналогии, метафоры, рифмы и юмор: они вызывают резонанс в этом поле. Похоже, она оценивает, насколько они  “звенят” - как правильно подобранные частоты на этой многомерной  мембране модели.

Итак, очень коротко, и плотно, что это даёт для понимания?
 1. Модель — не “знает”, она “вибрирует”. Её знание — не точка, а область вероятностей.
 2. Контекст — как возмущение поля. Он влияет на форму волны и сдвигает вероятности.
 3. Обработка — интерференционный процесс. Результат — не сумма, а суперпозиция смыслов.
 4. Обучение — настройка резонансных частот. Параметры модели настраиваются так, чтобы “на заданный ввод” возникала “нужная форма волны”.

Все. 
Выдыхай.
Айда покурим про это.